# SEO

* Robots：告诉机器人哪些页面可被爬取，哪些不可被爬取。
* Sitemap: 告诉爬虫站点有哪些页面。
* OG Image: 
* Schema.org
* Link Checker
* SEO Utils
* Skew Protection


## 控制爬虫

* 提高自然流量
    * 把有用内容告诉搜索引擎索引，自然能够更加精准知道站点定位。
* 内容保护
    * 大多数站点通常有多个环境，例如： 测试、预发和生产。
    * 默认情况下，搜索引擎将索引它们可以访问的任何公共环境，这可能导致重复的内容问题，并使最终用户在出现这些内容时感到困惑。
    * 页面需要权限才能访问，有时也会被搜索引擎索引。
* 降低服务负载
    * 限制哪些不能被访问，自然访问站点请求变少，请求变少自然降低服务负载。


## 其他链接

[nuxtseo](https://nuxtseo.com/)
[og](https://ogp.me/) : 用于网页优化分享效果，特别是在微信、QQ、微博、Facebook、Twitter等社交平台。当你的页面被这些平台抓取后，将会以这些属性内容展示。



## Google

谷歌搜索的3个阶段

* 抓取（Googlebot 程序）：Google 会使用名为“抓取工具”的自动程序从互联网上发现各类网页，并下载其中的文本、图片和视频。
* 索引编制：Google 会分析网页上的文本、图片和视频文件，并将信息存储在大型数据库 Google 索引中。
* 呈现搜索结果：当用户在 Google 中搜索时，Google 会返回与用户查询相关的信息。


### 抓取

能否抓取取决于 Google 的抓取工具能否访问网站。Googlebot 访问网站时的一些常见问题包括：

* 服务器在处理网站时出现问题
* 网络问题
* robots.txt 规则阻止 Googlebot 访问网页


### 索引编制

#### 什么是索引编制

抓取网页后，``Google`` 会尝试了解该网页的内容。这一阶段称为 “索引编制”，包括处理和分析文字内容以及关键内容标记和属性，例如 ``<title>`` 元素和 ``Alt`` 属性、``图片``、``视频``等。

在索引编制过程中， Google 会确定网页是否``与互联网上的其他网页重复或是否为规范网页``。规范网页是可能会显示在搜索结果中的网页。

``选择规范网页过程：``

* 我们首先会将在互联网上找到的内容类似的网页归为一组（也称为聚类），然后从中选择最具代表性的网页。该组网页中的其他网页可作为备用版本在不同情况下提供，例如用户在移动设备上进行搜索时，或他们正在查找该组网页中的某个具体网页时。
    * 内容很重
* Google 还会收集关于规范网页及其内容的信号，这些信号可能会在下一阶段（即在搜索结果中呈现网页）时用到。一些信号包括网页语言、内容所针对的国家/地区、网页易用性。
    * 语言很重要(多语言)
    * 易用性很重要(支持移动设备等)

### 呈现搜索结果

用户输入查询时，我们的机器会在索引中搜索匹配的网页，并返回我们认为与用户的搜索内容最相关的优质结果。相关性是由数百个因素决定的，其中可能包括用户的位置、语言和设备（桌面设备或手机）等信息。例如，在用户搜索“自行车维修店”后，Google 向巴黎用户显示的结果与向香港用户显示的结果有所不同。

``Search Console`` 可能提示您某个网页已编入索引，但您在搜索结果中看不到该网页。 这可能是因为

* 网页内容与用户查询无关
* 内容质量低
* ``Robots meta`` 规则阻止提供内容








